{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following notebook describes the process in reaching our final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.stats import mstats\n",
    "import re\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import datetime as dt\n",
    "import math\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "import numpy as np \n",
    "import sklearn.preprocessing as Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Calendar Dataset\n",
    "\n",
    "The calendar dataset gives the price of listings over the course of a year. The dataset was pulled from: http://data.beta.nyc/dataset/inside-airbnb-data/resource/ce0cbf46-83f9-414a-8a1d-7fd5321d83ca. In order to clean this dataset, we first renamed the columns with more appropriate labels. We then stripped the dates and prices of their extraneous characters so they were easier to work with. Finally, we converted the column types to apropriate dtypes and filtered out columns with no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open calendar csv file\n",
    "calendar = pd.read_csv(\"C:/Users/fahmida/Desktop/Rental/airbnb/calendar/calendar_jan_2019.csv\") \n",
    "\n",
    "# change column contents to be more workable format\n",
    "calendar['date'] = calendar['date'].map(lambda x: x.lstrip('\"').rstrip('\"'))\n",
    "calendar['price'] = calendar['price'].map(lambda x: x.lstrip('$').rstrip('.'))\n",
    "\n",
    "# change column dtypes \n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "calendar.rename(columns={'listing_id':'listing'}, \n",
    "                 inplace=True)\n",
    "\n",
    "# filter out the columns with no prices \n",
    "calendar=calendar[calendar['available'] == 't']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Listings Dataset\n",
    "\n",
    "The listings dataset is our main dataset for this project. It contains thousands of rows of listing information for Airbnbs in New York City. We attempted to clean the data many different ways before settling on our final dataset. One notable technique we tried was KNN. Ultimately, this was unsuccessful because most rows had some NaN values and it was therefore difficult to find similar listings. KNN ended up being too costly and ineffective.\n",
    "\n",
    "Below, we outline the process that ended up working best for us: \n",
    "First, we dropped the columns with null values that added no values to the dataset. We then dropped null values that could not be salvaged (things that could not be filled in based off of any known technique or our intuition such as ID or property type).\n",
    "Next, we changed the format of two variables (price and extra people) to integers rather than objects so we could easily perform statistical procedures with the information provided. The zipcodes were given in their extended form, so we decided to only use the first five numbers.\n",
    "For missing weekly and monthly prices and regular prices, we used scaled nightly price to fill in the missing data. Also, for all the pricing data, we converted the format to float values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2714: DtypeWarning: Columns (88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# open dataset with information and pricing for each listing \n",
    "df = pd.read_csv('C:/Users/fahmida/Desktop/Rental/airbnb/Listings/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with null values that don't add important information to dataset\n",
    "df = df.drop(['country', 'neighbourhood', 'square_feet', 'state'], 1)\n",
    "\n",
    "# drop null values that can't be salvaged \n",
    "df = df[(pd.notnull(df['id']))&(pd.notnull(df['host_id']))&(pd.notnull(df['zipcode']))&(pd.notnull(df['latitude']))]\n",
    "df = df[(pd.notnull(df['longitude']))&(pd.notnull(df['bathrooms']))&(pd.notnull(df['bedrooms']))&(pd.notnull(df['beds']))]\n",
    "df = df[(pd.notnull(df['property_type']))&(pd.notnull(df['price']))&((df.number_of_reviews!=0)&(pd.notnull(df.review_scores_rating)))]\n",
    "\n",
    "# reset index after dropping certain rows\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# change format of prices and extra_people to integer rather than object\n",
    "prices = []\n",
    "extra_people = []\n",
    "for i in range(len(df)):\n",
    "    price = int(float(str(df['price'][i]).replace('$', '').replace(',', '')))\n",
    "    extra_person = int(float(str(df['extra_people'][i]).replace('$', '').replace(',', '')))\n",
    "    prices.append(price)\n",
    "    extra_people.append(extra_person)\n",
    "\n",
    "df['price'] = prices\n",
    "df['extra_people'] = extra_people\n",
    "\n",
    "# only keep first five numbers of zipcode \n",
    "zipcodes = []\n",
    "for i in range(len(df)):\n",
    "    zipcode = df['zipcode'][i][:5]\n",
    "    zipcodes.append(zipcode)\n",
    "df['zipcode'] = zipcodes\n",
    "\n",
    "# fill the null values in weekly_prices and monthly_prices column\n",
    "# change the dtype from object to integer in these two columns\n",
    "wprices = []\n",
    "mprices = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if pd.isnull(df['weekly_price'][i]):\n",
    "        wprice = df['price'][i] * 7\n",
    "    elif pd.notnull(df['weekly_price'][i]):\n",
    "        wprice = int(float(str(df['weekly_price'][i]).replace('$', '').replace(',', '')))\n",
    "    wprices.append(wprice)\n",
    "    if pd.isnull(df['monthly_price'][i]):\n",
    "        mprice = df['price'][i] * 30\n",
    "    elif pd.notnull(df['monthly_price'][i]):\n",
    "        mprice = int(float(str(df['monthly_price'][i]).replace('$', '').replace(',', '')))\n",
    "    mprices.append(mprice)\n",
    "    \n",
    "df['weekly_price'] = wprices\n",
    "df['monthly_price'] = mprices\n",
    "\n",
    "df = df.dropna(axis=0)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed sentiment analysis using a dataset that provided reviews on different listings. More details about sentiment analysis are shown in another notebook, as there is a good deal of cleaning, exploration, and analysis that occurred on that notebook alone, but here we show how sentiment scores were included into our master dataset. Additionally, we added the 'host_since', 'first_review', and 'last_review' columns from the sentiment dataset, because in that dataset cleaning, we had altered these columns (rather than dates, we made them \"time\", i.e.: seconds, to so that they were in integer, rather than object, format). More to come in that notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File time_sentiment.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0ce6718e6bdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# add in new csv with the sentiment columns and converted host_since, first_review, last_review columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time_sentiment.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mhost_since\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'host_since'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File time_sentiment.csv does not exist"
     ]
    }
   ],
   "source": [
    "# drop the original host_since, first_review, last_review\n",
    "df = df.drop(['host_since', 'first_review', 'last_review'], 1)\n",
    "\n",
    "# add in new csv with the sentiment columns and converted host_since, first_review, last_review columns\n",
    "sentiment = pd.read_csv('time_sentiment.csv')\n",
    "\n",
    "host_since = sentiment['host_since'] \n",
    "first_review = sentiment['first_review'] \n",
    "last_review = sentiment['last_review'] \n",
    "sentiments = sentiment['sentiment']\n",
    "\n",
    "frames = [host_since, first_review, last_review, sentiments]\n",
    "final_sentiment = pd.concat(frames, 1)\n",
    "final_sentiment = final_sentiment.dropna(axis=0, how='any')\n",
    "frames_new = [df, final_sentiment]\n",
    "\n",
    "df_final = pd.concat(frames_new, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating Indicator for Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We later explain some explorations that were done on our calendar dataset. However, here we show how we added a column of indicators to the master dataset for each holiday; a listing received a \"1\" on a certain column if its price fluctuated on the day of the holiday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of most important holidays \n",
    "unique_ids = df_final['id'].unique()\n",
    "unique_dates = ['2019-01-01', '2018-07-04', '2018-12-25', '2018-12-31']\n",
    "\n",
    "# empty arrays which we will add our indicators to \n",
    "new_years = []\n",
    "july_4th = []\n",
    "christmas = []\n",
    "eve = []\n",
    "holidays = [new_years, july_4th, christmas, eve]\n",
    "\n",
    "# add an indicator for each row if a listing's price changed on any of these holidays \n",
    "for i in range(len(unique_ids)):\n",
    "    real_price = df_final['price'][i]\n",
    "    list_prices = calendar[calendar['listing'] == unique_ids[i]]\n",
    "    # indicator column for each holiday\n",
    "    for j in range(len(unique_dates)):\n",
    "        price_holiday = list_prices['price'][list_prices['date'] == unique_dates[j]]\n",
    "        if len(price_holiday) > 0:\n",
    "            # check if price changes on that holiday \n",
    "            if price_holiday.iloc[0] != real_price:\n",
    "                holidays[j].append(1)\n",
    "            else:\n",
    "                holidays[j].append(0)\n",
    "        else:\n",
    "            holidays[j].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of indicators for each holiday to the dataset\n",
    "df_final['New Years'] = new_years\n",
    "df_final['July 4th'] = july_4th\n",
    "df_final['Christmas'] = christmas\n",
    "df_final['New Years Eve'] = eve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding \"Nearest Attractions\" Feature\n",
    "\n",
    "More details are shown in a separate notebook, but here we add a count of the best attractions in NY that the listing is close to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractions = pd.read_csv('attractions_added.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractions = attractions['attraction_count']\n",
    "df_final['attraction_count'] = attractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of Certain Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate dataset, after an iteration of tuning models, we noticed a few features that were skewing our results because they were not standardized. Hence, we added this step of 'feature engineering' to our final dataset, in which we standardized/winsorized certain features in our master dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the new data\n",
    "for feat in ['host_since','first_review','last_review']:\n",
    "    df_final[feat] = (df_final[feat] - df_final[feat].mean()) / (df_final[feat].max() - df_final[feat].min())\n",
    "df_final['maximum_nights'] = mstats.winsorize(df_final['maximum_nights'],limits=(0,0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataset!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the columns in our master dataset\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('airbnb_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we found numerous outliers in the dataset, we believed it would be better to take the log of prices so that prices could be plotted on a more normal distribution. This would also allow us to see relationships between variables and price better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_price = df_final.price.apply(math.log)\n",
    "df['log_price'] = log_price\n",
    "df['log_price'].to_csv('log_prices.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
